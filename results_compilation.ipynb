{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "# sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Performance over iterations \n",
    "\n",
    "## Visualize Frechet Inception Distance per Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = pd.read_csv(\"logs/Aug14_08-04-23_overdrive/Aug14_08-04-23_overdrive.csv\")\n",
    "model2 = pd.read_csv(\"logs/Aug15_20-22-52_overdrive/Aug15_20-22-52_overdrive.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.lineplot(data=model2, x='Step', y='Value')\n",
    "sns.lineplot(data=model1, x='Step', y='Value')\n",
    "plt.plot([200]*list(model1[\"Step\"])[-1], \"r--\")\n",
    "plt.legend([\"CMSGGAN with One-hot-Encodings (RA Hinge)\", \"CMSGGAN with Learned Embeddings (RA Hinge)\"])\n",
    "plt.title(\"Frechet Inception Distance over training iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"FID\")\n",
    "plt.savefig(\"logs/FIDs.jpg\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Evaluation\n",
    "\n",
    "## Visualize Generated Images per Gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_grid(results_dir, classes=\"all\", figsize=(6,50), save_as=None):\n",
    "    plt.close()\n",
    "    \n",
    "    cols = ['Image {}'.format(i+1) for i in range(5)]\n",
    "    if classes == \"all\":\n",
    "        rows = os.listdir(results_dir)\n",
    "    else:\n",
    "        rows = classes\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = fig.add_gridspec(len(rows), len(cols), hspace=0, wspace=0)\n",
    "    axes = gs.subplots(sharex='col', sharey='row')\n",
    "    axes = axes[None, :] if len(rows) == 1 else axes\n",
    "\n",
    "    for ax, col in zip(axes[0], cols):\n",
    "        ax.set_title(col)\n",
    "        \n",
    "    for ax, row in zip(axes[:,0], rows):\n",
    "        ax.set_ylabel(row, rotation=90, size='large')\n",
    "        \n",
    "    for i, c in enumerate(rows):\n",
    "        images_list = sorted(os.listdir(os.path.join(results_dir, c)))[:5]\n",
    "        for j, f in enumerate(images_list):\n",
    "            img = plt.imread(os.path.join(results_dir, c, f))\n",
    "            axes[i, j].imshow(img, cmap=plt.cm.gray)\n",
    "            axes[i, j].set_yticklabels([])\n",
    "            axes[i, j].set_xticklabels([])\n",
    "            axes[i, j].tick_params(left=False, bottom=False)\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"classes.txt\", 'r') as f:\n",
    "    classes = f.read().splitlines()\n",
    "results_dir = \"results/data:all_baf_valid_50deg_filtered3.csv_classes:classes.txt_trans:256-1-1_mod:cmsggan-512-256_tr:1000-RAHinge-48-1-0.003-0.003-0.0-0.99/model_ema_state_100/generated_examples\"\n",
    "cl = [\"ABCA4\", \"USH2A\", \"RPGR\", \"BEST1\", \"PRPH2\", \"RS1\", \"TIMP3\", \"PROML1\"]\n",
    "create_image_grid(results_dir, classes=cl, figsize=(7.5, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"classes.txt\", 'r') as f:\n",
    "    classes = f.read().splitlines()\n",
    "results_dir = \"results/data:all_baf_valid_50deg_filtered3.csvclasses:classes.txt_trans:256-1-1_mod:cmsgganv2-512-256_tr:1000-RAHinge-48-1-0.003-0.003-0.0-0.99/model_ema_state_350/generated_examples\"\n",
    "cl = [\"ABCA4\", \"USH2A\", \"RPGR\", \"BEST1\", \"PRPH2\", \"RS1\", \"TIMP3\", \"PROML1\"]\n",
    "create_image_grid(results_dir, classes=cl, figsize=(7.5, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Evaluation\n",
    "\n",
    "## Compute class predictions by Eye2Gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_confidence(imgs, return_labels=True):\n",
    "\n",
    "    # install libraries\n",
    "    import tensorflow as tf\n",
    "    import json\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    \n",
    "    # load images\n",
    "    if isinstance(imgs, (torch.FloatTensor, np.ndarray)):\n",
    "        images = imgs[:, :, :, None].repeat(3, -1)\n",
    "    else:\n",
    "        images = np.zeros((len(imgs), 256, 256, 3))\n",
    "        for i, img in enumerate(imgs):\n",
    "            images[i, :, :, :] = plt.imread(img)[:, :, None].repeat(3, -1)\n",
    "            \n",
    "    preprocess_func = tf.keras.applications.inception_v3.preprocess_input\n",
    "    images = preprocess_func(images)\n",
    "\n",
    "    # load pretrained eye2gene classifier\n",
    "    from models.eye2gene.base import Model\n",
    "    model_paths = os.listdir(\"models/eye2gene/weights/\")\n",
    "    model_paths = [os.path.join(\"models/eye2gene/weights\", path) for path in model_paths if path.endswith(\".h5\")]\n",
    "    \n",
    "    conf = np.zeros((50, 36))\n",
    "    for path in model_paths:\n",
    "        model = Model().load(path)\n",
    "        conf += model.predict(images)\n",
    "    conf = np.divide(conf, len(model_paths))\n",
    "    \n",
    "    # create index to labels converter\n",
    "    config_path = model_paths[0][:-3] + '.json'\n",
    "    with open(config_path, 'r') as config_file:\n",
    "        model_config = json.load(config_file)\n",
    "    \n",
    "    df = pd.DataFrame(conf, columns=model_config['classes'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_dir = \"results/data:all_baf_valid_50deg_filtered3.csv_classes:classes.txt_trans:256-1-1_mod:cmsggan-512-256_tr:1000-RAHinge-48-1-0.003-0.003-0.0-0.99/model_ema_state_100/generated_examples\"\n",
    "\n",
    "confidences = []\n",
    "for c in sorted(os.listdir(results_dir)):\n",
    "    plt.close()\n",
    "    print(c)\n",
    "    images = sorted(os.listdir(os.path.join(results_dir, c)))\n",
    "    images = [os.path.join(results_dir, c, images[i]) for i in range(len(images))]\n",
    "    confidences.append(compute_class_confidence(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidences_2 = pd.concat([confidences[i].idxmax(axis=1) for i in range(len(confidences))], axis=1)\n",
    "confusions = pd.DataFrame(0, index=sorted(os.listdir(results_dir)), columns=sorted(os.listdir(results_dir)))\n",
    "for i, col in enumerate(confusions.columns):\n",
    "    confusions[col] = confusions[col].combine(confidences_2[i].value_counts(normalize=0), max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(confusions, annot=True)\n",
    "plt.xlabel(\"Actual Class of Image\")\n",
    "plt.ylabel(\"Predicted Class by Eye2Gene\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Generate completely random images and study confusions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.data_utils import ImageDataset\n",
    "from generators.msggan_gen import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls, sizes = np.unique(dataset.img_labels, return_counts=True)\n",
    "cls[np.argsort(sizes)][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1399)\n",
    "torch.cuda.manual_seed(1399)\n",
    "\n",
    "latent_size = 512\n",
    "n_samples = 50\n",
    "resolution = 256\n",
    "dataset = ImageDataset('datasets/all_baf_valid_50deg_filtered3.csv',\n",
    "                       'file.path',\n",
    "                       'gene',\n",
    "                       'classes.txt')\n",
    "classes = dataset.classes\n",
    "class_mapping = dataset.class2idx\n",
    "generate_randomly = True\n",
    "weights_path = \"checkpoints/data:all_baf_valid_50deg_filtered3.csv_classes:classes.txt_trans:256-1-1_mod:cmsggan-512-256_tr:1000-RAHinge-48-1-0.003-0.003-0.0-0.99/model_ema_state_100.pth\"\n",
    "model_name = \"cmsggan\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "generated_images = generate(latent_size,\n",
    "                            n_samples,\n",
    "                            resolution, \n",
    "                            True, \n",
    "                            classes,\n",
    "                            class_mapping,\n",
    "                            generate_randomly,\n",
    "                            weights_path,\n",
    "                            model_name,\n",
    "                            device)\n",
    "generated_images = generated_images*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_dir = \"results/data:all_baf_valid_50deg_filtered3.csv_classes:classes.txt_trans:256-1-1_mod:cmsggan-512-256_tr:1000-RAHinge-48-1-0.003-0.003-0.0-0.99/model_ema_state_100/generated_examples\"\n",
    "confidences = []\n",
    "for c in classes:\n",
    "    print(c)\n",
    "    idx = class_mapping[c]\n",
    "    confidences.append(compute_class_confidence(generated_images[idx, :, :, :].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidences_2 = pd.concat([confidences[i].idxmax(axis=1) for i in range(len(confidences))], axis=1)\n",
    "confusions = pd.DataFrame(0, index=classes, columns=classes)\n",
    "for i, col in enumerate(confusions.columns):\n",
    "    confusions[col] = confusions[col].combine(confidences_2[i].value_counts(normalize=0), max)\n",
    "    \n",
    "plt.close()\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(confusions, annot=True)\n",
    "plt.xlabel(\"Actual Class of Image\")\n",
    "plt.ylabel(\"Predicted Class by Eye2Gene\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize FID scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.evaluate import compute_fid_eye2gene\n",
    "from helpers.data_utils import ImageDataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fid_eye2gene(gen_imgs, real_imgs):\n",
    "    \"\"\" Computes frechet inception distance using our Eye2Gene pretrained weights model \"\"\"\n",
    "\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "    import tensorflow as tf\n",
    "    import scipy\n",
    "\n",
    "    def preprocess(imgs):\n",
    "        \"\"\" Resizes tensors for Eye2gene model \"\"\"\n",
    "        imgs = imgs.detach().numpy()\n",
    "        imgs = imgs[:, :, :, None]\n",
    "        imgs = np.repeat(imgs, 3, -1)\n",
    "        return imgs\n",
    "\n",
    "    # separate preprocess function just for inceptionv3\n",
    "    inception_preprocess_func = tf.keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    # preprocess generated and real images\n",
    "    gen_imgs, real_imgs = preprocess(gen_imgs), preprocess(real_imgs)\n",
    "    gen_imgs, real_imgs = inception_preprocess_func(gen_imgs), inception_preprocess_func(real_imgs)\n",
    "\n",
    "    # load model\n",
    "    model_paths = os.listdir(\"models/eye2gene/weights/\")\n",
    "    model_paths = [os.path.join(\"models/eye2gene/weights\", path) for path in model_paths if path.endswith(\".h5\")]\n",
    "    \n",
    "    inception_model_full = tf.keras.applications.InceptionV3(include_top=True,\n",
    "                                                             classes=36,\n",
    "                                                             weights=None,\n",
    "                                                             input_shape=(256, 256, 3),\n",
    "                                                             pooling='max')\n",
    "    inception_model = tf.keras.Model(inputs=inception_model_full.input, outputs=inception_model_full.layers[-2].output)\n",
    "\n",
    "    # helper functions\n",
    "    def matrix_sqrt(x):\n",
    "        y = scipy.linalg.sqrtm(x)\n",
    "        if np.iscomplexobj(y):\n",
    "            y = y.real\n",
    "        return y\n",
    "\n",
    "    def frechet_distance(mu_x, mu_y, sigma_x, sigma_y):\n",
    "        return (mu_x - mu_y).dot(mu_x - mu_y) + np.trace(sigma_x) + np.trace(sigma_y) - \\\n",
    "               2*np.trace(matrix_sqrt(sigma_x @ sigma_y))\n",
    "\n",
    "    def get_covariance(features):\n",
    "        return np.cov(features, rowvar=False)\n",
    "\n",
    "    # ============================\n",
    "    # Get the image features\n",
    "    # ============================\n",
    "    gen_features_all = np.zeros((50, 2048))\n",
    "    real_features_all = np.zeros((50, 2048))\n",
    "    for path in model_paths:\n",
    "        inception_model_full.load_weights(path)\n",
    "        gen_features_all += inception_model.predict(gen_imgs)\n",
    "        real_features_all += inception_model.predict(real_imgs)\n",
    "        \n",
    "    gen_features_all = gen_features_all / 5\n",
    "    real_features_all = real_features_all / 5\n",
    "    \n",
    "    # ============================\n",
    "    # Calculate feature statistics\n",
    "    # ============================\n",
    "\n",
    "    # calculate mean across all observations\n",
    "    mu_fake = gen_features_all.mean(0)\n",
    "    mu_real = real_features_all.mean(0)\n",
    "\n",
    "    # calculate covariance\n",
    "    sigma_fake = get_covariance(gen_features_all)\n",
    "    sigma_real = get_covariance(real_features_all)\n",
    "\n",
    "    fid = frechet_distance(mu_real, mu_fake, sigma_real, sigma_fake)\n",
    "\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_imgs = torch.load(\"gen_imgs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1399)\n",
    "np.random.seed(1399)\n",
    "# load real images\n",
    "image_transforms = []\n",
    "# image resizing\n",
    "image_transforms.append(transforms.Resize((256, 256)))\n",
    "image_transforms.append(transforms.Grayscale())\n",
    "image_transforms.append(transforms.ToTensor())\n",
    "image_transforms = transforms.Compose(image_transforms)\n",
    "data = ImageDataset(\"datasets/all_baf_valid_50deg_filtered3.csv\", \"file.path\", \"gene\", [\"ABCA4\"], image_transforms)\n",
    "real_samples = data.get_samples(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.64360094354555"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abca4_fid = compute_fid_eye2gene(generated_imgs[0]*255, real_samples*255)\n",
    "abca4_fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"classes.txt\", 'r') as f:\n",
    "    classes = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1\n",
    "FID_cmsggan1_imagenet = pd.read_csv(\"results/data:all_baf_valid_50deg_filtered3.csv_classes:classes.txt_trans:256-1-1_mod:cmsggan-512-256_tr:1000-RAHinge-48-1-0.003-0.003-0.0-0.99/model_ema_state_100/metrics/fid_imagenet/fid_imagenet_scores_per_class.csv\")\n",
    "FID_cmsggan1_eye2gene = pd.read_csv(\"results/data:all_baf_valid_50deg_filtered3.csv_classes:classes.txt_trans:256-1-1_mod:cmsggan-512-256_tr:1000-RAHinge-48-1-0.003-0.003-0.0-0.99/model_ema_state_100/metrics/fid_eye2gene/fid_eye2gene_scores_per_class.csv\")\n",
    "# model 2\n",
    "FID_cmsggan2_imagenet = pd.read_csv(\"results/data:all_baf_valid_50deg_filtered3.csvclasses:classes.txt_trans:256-1-1_mod:cmsgganv2-512-256_tr:1000-RAHinge-48-1-0.003-0.003-0.0-0.99/model_ema_state_350/metrics/fid_imagenet/fid_imagenet_scores_per_class.csv\")\n",
    "FID_cmsggan2_eye2gene = pd.read_csv(\"results/data:all_baf_valid_50deg_filtered3.csvclasses:classes.txt_trans:256-1-1_mod:cmsgganv2-512-256_tr:1000-RAHinge-48-1-0.003-0.003-0.0-0.99/model_ema_state_350/metrics/fid_eye2gene/fid_eye2gene_scores_per_class.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_1\n",
    "FID_cmsggan1_imagenet[\"log_FID\"] = np.log(FID_cmsggan1_imagenet[\"0\"])\n",
    "FID_cmsggan1_eye2gene[\"log_FID\"] = np.log(FID_cmsggan1_eye2gene[\"0\"])\n",
    "# model 2 \n",
    "FID_cmsggan2_imagenet[\"log_FID\"] = np.log(FID_cmsggan2_imagenet[\"0\"])\n",
    "FID_cmsggan2_eye2gene[\"log_FID\"] = np.log(FID_cmsggan2_eye2gene[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [FID_cmsggan1_imagenet['Unnamed: 0'],\n",
    "              FID_cmsggan1_imagenet[\"log_FID\"], \n",
    "              FID_cmsggan1_eye2gene[\"log_FID\"],\n",
    "              FID_cmsggan2_imagenet[\"log_FID\"], \n",
    "              FID_cmsggan2_eye2gene[\"log_FID\"]]\n",
    "metrics = pd.concat(dataframes, axis=1, ignore_index=True)\n",
    "metrics.columns = [\"Gene\", \"FID_cmsggan1_imagenet\", \"FID_cmsggan1_eye2gene\", \"FID_cmsggan2_imagenet\", \"FID_cmsggan2_eye2gene\"]\n",
    "metrics = pd.melt(metrics, id_vars=[\"Gene\"], value_vars=metrics.columns[1:])\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "g = sns.FacetGrid(metrics, row=\"variable\", height=2, aspect=5)\n",
    "g.map(sns.barplot, \"Gene\", \"value\", order=classes)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.title(\"FID (ImageNet) - CMSGGAN-1\")\n",
    "sns.barplot(data=FID_cmsggan1_imagenet, x=\"Unnamed: 0\", y=\"0\")\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False)\n",
    "plt.xlabel(None)\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.title(\"FID (Eye2Gene) - CMSGGAN-1\")\n",
    "sns.barplot(data=FID_cmsggan1_eye2gene, x=\"Unnamed: 0\", y=\"0\")\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False)\n",
    "plt.xlabel(None)\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.title(\"FID (ImageNet) - CMSGGAN-2\")\n",
    "sns.barplot(data=FID_cmsggan2_imagenet, x=\"Unnamed: 0\", y=\"0\")\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False)\n",
    "plt.xlabel(None)\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.title(\"FID (Eye2Gene) - CMSGGAN-2\")\n",
    "sns.barplot(data=FID_cmsggan2_eye2gene, x=\"Unnamed: 0\", y=\"0\")\n",
    "\n",
    "plt.xlabel(\"Gene\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize MI scores and most similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mutual_info(results_dir, figsize=(5, 1)):\n",
    "    \n",
    "    rows = 36\n",
    "    cols = 1\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "#     plt.figure(figsize=figsize)\n",
    "    images_list = os.listdir(results_dir)\n",
    "    images_list = [os.path.join(results_dir, img) for img in images_list if \"MI_hist_\" in img]\n",
    "    \n",
    "    for i in range(len(images_list)):\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        plt.imshow(plt.imread(images_list[i]))\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "#     gs = fig.add_gridspec(rows, cols, hspace=0, wspace=0)\n",
    "#     axes = gs.subplots(sharex='col', sharey='row')     \n",
    "#     for i, c in enumerate(rows):\n",
    "#         for j, f in enumerate(images_list):\n",
    "#             img = plt.imread(images_list[])\n",
    "#             axes[i, j].imshow(img, cmap=plt.cm.gray)\n",
    "#             axes[i, j].set_yticklabels([])\n",
    "#             axes[i, j].set_xticklabels([])\n",
    "\n",
    "#     plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
